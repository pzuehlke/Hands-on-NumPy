{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.linalg as spla\n",
    "import scipy.optimize as spo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving equations\n",
    "\n",
    "Linear systems of equations arise naturally in many contexts in engineering and\n",
    "science.  NumPy, especially through its `numpy.linalg` submodule, offers several\n",
    "methods to compute exact or approximate solutions to such systems, and to solve\n",
    "related problems such as finding the eigenvalues and eigenvectors of a matrix or\n",
    "determining its various decompositions. In this notebook we'll also learn how to\n",
    "use another fundamental library in the Python ecosystem called __SciPy__ that\n",
    "can help us solve nonlinear equations and optimization problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $ \\S 1 $ Square linear systems\n",
    "\n",
    "### $ 1.1 $ Basic terminology\n",
    "\n",
    "Consider a set of $ n $ linear equations in $ n $ unknowns $ x_1, \\dots, x_n $:\n",
    "\\begin{equation*}\n",
    "\\begin{cases}\n",
    "& a_{11} x_1 &+& a_{12}x_2 &+& \\cdots &+& a_{1n}x_n &=& b_1 \\\\\n",
    "& a_{21} x_1 &+& a_{22}x_2 &+& \\cdots &+& a_{2n}x_n &=& b_2 \\\\\n",
    "& \\vdots &+& \\vdots &+& \\cdots &+& \\vdots &=&\\vdots \\\\\n",
    "& a_{n1} x_1 &+& a_{n2}x_2 &+& \\cdots &+& a_{nn}x_n &=& b_n\n",
    "\\end{cases}\n",
    "\\end{equation*}\n",
    "\n",
    "Such equations are called __linear__ because the expressions on the left\n",
    "side are linear in the variables, i.e., they do not involve powers such as\n",
    "$ x_1^3 $ nor products such as $ x_1x_2 $ nor more complicated\n",
    "functions of the variables such as $ \\cos(x_1) $ or $ e^{x_2} $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equivalently, using matrix notation, this system of equations can be rewritten as:\n",
    "\\begin{equation*}\n",
    "A\\,\\mathbf x =\n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12} & \\cdots & a_{1n} \\\\\n",
    "a_{21} & a_{22} & \\cdots & a_{2n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "a_{n1} & a_{n2} & \\cdots & a_{nn}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "\\vdots \\\\\n",
    "x_n\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "b_1 \\\\\n",
    "b_2 \\\\\n",
    "\\vdots \\\\\n",
    "b_n\n",
    "\\end{bmatrix} = \\mathbf b\n",
    "\\end{equation*}\n",
    "or more concisely\n",
    "$$\n",
    "\\boxed{\\ A\\,\\mathbf{x} = \\mathbf{b}\\ }\n",
    "$$\n",
    "\n",
    "The matrix $ A $ is called the __coefficient matrix__.  Notice that in our\n",
    "case, it is _square_ (i.e., the number of lines is the same as the\n",
    "number of columns). Systems of $ m $ linear equations in $ n $ unknowns where\n",
    "$ m \\ne n $ will be considered only in the next section, for the sake of\n",
    "simplicity.\n",
    "\n",
    "üìù Depending on the properties of $ A $ and $ \\mathbf{b} $, a linear system\n",
    "$ A\\,\\mathbf{x} = \\mathbf{b} $ may have exactly one solution, no solutions, or\n",
    "infinitely many solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise:__ Write down as NumPy arrays the coefficient matrices $ A $ and the\n",
    "vectors $ \\mathbf x $ and $ \\mathbf b $ for the systems of linear equations\n",
    "below:\n",
    "\n",
    "(a) $$ \\left\\{\n",
    "\\begin{align*}\n",
    "2x &+ 3y &\\ =& \\ 5 \\\\\n",
    "4x &- \\ y &\\ =& \\ 1\n",
    "\\end{align*} \\right.$$\n",
    "\n",
    "(b)\n",
    "$$\n",
    "\\left\\{\\begin{align*}\n",
    "x &\\ \\ +\\ &2y & \\ \\ - & z & \\ = & \\ 4 \\\\\n",
    "2x &\\ \\ - &y & \\ \\ + &3z & \\ = & -6 \\\\\n",
    "-3x &\\ \\ + &4y &\\ \\ + & z & \\ = & 10\n",
    "\\end{align*} \\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __rank__ of a matrix is the maximum number of linearly independent columns\n",
    "or, equivalently, rows in the matrix. We can compute the rank of a matrix in\n",
    "NumPy with `linalg.matrix_rank`. For example, the first two columns of the\n",
    "matrix below are clearly independent, but the third column is just the\n",
    "sum of the first two, hence the rank of the matrix is $ 2 $:\n",
    "$$\n",
    "    A = \\begin{bmatrix}\n",
    "    1 & -1 & \\phantom{-}0 \\\\\n",
    "    2 & -3 & -1 \\\\\n",
    "    0 & \\phantom{-}1 & \\phantom{-}1\n",
    "    \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1, -1,  0],\n",
    "              [2, -3, -1],\n",
    "              [0,  1,  1]])\n",
    "\n",
    "# Verify the rank:\n",
    "rank = np.linalg.matrix_rank(A)\n",
    "print(f\"Rank of A: {rank}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $ 1.2 $ Solving square linear systems of equations\n",
    "\n",
    "__Example:__ Consider the system of equations:\n",
    "$$\n",
    "\\left\\{\\begin{alignat*}{4}\n",
    "3x\\  &+\\ 2y\\ &&-\\ &z\\ &=&\\ 1\\\\\n",
    "2x\\ &-\\ 2y\\ &&+\\ 4&z\\ &=&\\ -2\\\\\n",
    "-x\\  &+\\ \\tfrac{1}{2}y\\ &&- &z\\ &=&\\ 0\n",
    "\\end{alignat*}\\right.\n",
    "$$\n",
    "or, in matrix form,\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\begin{bmatrix}\n",
    "3 & 2 & -1 \\\\\n",
    "2 & -2 & 4 \\\\\n",
    "-1 & 0.5 & -1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x \\\\\n",
    "y \\\\\n",
    "z\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\phantom{-}1 \\\\\n",
    "-2 \\\\\n",
    "\\phantom{-}0\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We can task NumPy with solving it by using the function `linalg.solve`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficient matrix A and constant vector b:\n",
    "A = np.array([[3, 2, -1],\n",
    "              [2, -2, 4],\n",
    "              [-1, 0.5, -1]])\n",
    "b = np.array([1, -2, 0])\n",
    "\n",
    "# Solve the system of equations:\n",
    "x = np.linalg.solve(A, b)\n",
    "print(x)  # Solution vector x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise:__ Verify by direct substitution that $ (1, -2, -2) $ is, in fact,\n",
    "the solution to the preceding equations. _Hint:_ Multiply $ A $ by $ (1, -2, -2)\n",
    "$ and check whether the result coincides with $ \\mathbf{b} $ (up to roundoff\n",
    "errors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise:__ Solve the following system of linear equations using NumPy and\n",
    "then verify the solution by substituting it back into $ A\\,\\mathbf{x} = \\mathbf{b} $:\n",
    "$$\n",
    "\\left\\{\\begin{alignat*}{4}\n",
    "2x_1\\  &-\\ x_2\\ &&+\\ 3&x_3\\ &=&\\ 5\\\\\n",
    "4x_1\\ &+\\ 4x_2\\ &&-\\ 3&x_3\\ &=&\\ 3\\\\\n",
    "-2x_1\\  &+\\ 5x_2\\ &&+ &x_3\\ &=&\\ 7\n",
    "\\end{alignat*}\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise:__ Let $ y = a_1 x + b_1 $ and $ y = a_2 x + b_2 $ describe two\n",
    "(non-vertical) lines.  Write a procedure `meeting_point` that takes as input\n",
    "these coefficients and returns the point where the two lines meet.\n",
    "Test your procedure on the lines\n",
    "$$\n",
    "\\begin{align*}\n",
    "    y &= 2x + 1 \\\\\n",
    "    y &= -x + 4\n",
    "\\end{align*}\n",
    "$$\n",
    "which intersect at $ (1, 3) $.\n",
    "_Hint:_ Convert the problem into that of solving linear equations and use the\n",
    "`linalg.solve` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise:__ Write a procedure `meeting_planes` that computes the point where\n",
    "three given planes \n",
    "$$\n",
    "\\begin{align*}\n",
    "    z &= a_1y + b_1x + c_1 \\\\\n",
    "    z &= a_2y + b_2x + c_2 \\\\\n",
    "    z &= a_3y + b_3x + c_3\n",
    "\\end{align*}\n",
    "$$\n",
    "meet. Test your code by verifying that the following three planes intersect at $ (-3, -5, -10) $:\n",
    "$$\n",
    "\\begin{align*}\n",
    "    z &= x + 2y + 3 \\\\\n",
    "    z &= 2x + y + 1 \\\\\n",
    "    z &= -x + 3y  + 2\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise:__ Consider a simple electrical circuit with three loops and three\n",
    "resistors. The loop currents $ I_1 $, $ I_2 $, and $ I_3 $ satisfy the following\n",
    "equations (derived from Kirchhoff's laws):\n",
    "$$\n",
    "\\left\\{\\begin{alignat*}{4}\n",
    "5I_1\\ &-\\ 2I_2\\ &&-\\ &I_3\\ &=&\\ 12\\\\\n",
    "-2I_1\\ &+\\ 8I_2\\ &&-\\ 3&I_3\\ &=&\\ -4\\\\\n",
    "-I_1\\ &-\\ 3I_2\\ &&+\\ 6&I_3\\ &=&\\ 6\n",
    "\\end{alignat*}\\right.\n",
    "$$\n",
    "where the right-hand sides represent voltage sources in the loops. Solve for the loop currents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $ 1.3 $ Additional observations on `linalg.solve`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise:__ Consider the following two linear system of equations. Try to\n",
    "solve them using `linalg.solve` and explain the output in each case.\n",
    "\n",
    "(a)\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\begin{bmatrix}\n",
    "1 & -2 \\\\\n",
    "-3 & 6 \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x \\\\\n",
    "y\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "3\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "(b) \n",
    "$$\n",
    "\\begin{align*}\n",
    "\\begin{bmatrix}\n",
    "1 & -2 \\\\\n",
    "-3 & 6 \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x \\\\\n",
    "y\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\phantom{-}1 \\\\\n",
    "-3\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "(c)\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\begin{bmatrix}\n",
    "1 & -2 & 3 \\\\\n",
    "-3 & 6 & 4 \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x \\\\\n",
    "y \\\\\n",
    "z\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "3\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "(d) Can you find (by hand) the solutions to the systems in items (a) and (b)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preceding exercise illustrates that:\n",
    "* `linalg.solve` should only be used on _square_ systems of linear equations\n",
    "  (where the number of equations is the same as the number of unknowns); otherwise\n",
    "  an error will be raised.\n",
    "* If the coefficient matrix is square but __singular__\n",
    "  (i.e., its rank is $ < n $ or equivalently its determinant equals zero), \n",
    "  then the corresponding system will not admit a unique solution: it may\n",
    "  have no solution at all or an infinite number of them. `linalg.solve` should\n",
    "  also not be used in this case, for it may raise an error or, even worse,\n",
    "  yield a wrong result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö° Under the hood, `linalg.solve` solves a (square) linear system $ A\\mathbf x =\n",
    "\\mathbf b $ by first finding the $ LU $ decomposition of $ A $, then rewriting\n",
    "the original system as\n",
    "$$\n",
    "\\left\\{\n",
    "\\begin{alignat*}{4}\n",
    "L\\,\\mathbf y &= \\mathbf b \\\\\n",
    "U\\,\\mathbf x &= \\mathbf y\n",
    "\\end{alignat*}\n",
    "\\right.\n",
    "$$\n",
    "Since $ L $ is lower triangular and $ U $ is upper triangular, the first of\n",
    "these systems can easily be solved by forward-substitution, and the second one\n",
    "by backward-substitution. To find the $ LU $ decomposition itself, Gaussian\n",
    "elimination is applied to $ A $. This is discussed in any Linear Algebra course,\n",
    "where you will do many of these computations by hand.\n",
    "\n",
    "The time complexity of finding the $ LU $ decomposition of (or, equivalently,\n",
    "performing Gaussian elimination on) an $ n \\times n $ matrix $ A $ is $ \\Theta(n^3)\n",
    "$, but once this decomposition is available, the complexity of solving\n",
    "$ A \\mathbf{x} = \\mathbf{b} $ in the way described above is only $ \\Theta(n^2) $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $ 1.4 $ Solving multiple square systems sharing the same coefficient matrix\n",
    "\n",
    "For systems with multiple right-hand sides, we can pass a matrix $ B $ as the second argument.\n",
    "More precisely, consider the linear system\n",
    "$$\n",
    "AX = B\n",
    "$$\n",
    "where $ A $ is a square $ n \\times n $ matrix as before, $ X $ is an $ n \\times\n",
    "p $ matrix of unknowns, and $ B $ is also an $ n \\times p $ matrix, containing\n",
    "multiple column vectors $ \\mathbf{b}_1,\\, \\mathbf{b}_2, \\cdots,\\, \\mathbf{b}_p $ in $\n",
    "\\mathbb{R}^n $. In other words, we want to simultaneously solve $ p $ systems of\n",
    "linear equations sharing the same coefficient matrix $ A $ but with different\n",
    "right-hand side vectors $ \\mathbf{b}_i $.  We can accomplish this using\n",
    "`linalg.solve` just as for a single $ \\mathbf{b} $:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the coefficient matrix A:\n",
    "A = np.array([[3, 1, -1],\n",
    "              [2, -2, 4],\n",
    "              [1, 5, -3]])\n",
    "\n",
    "# The columns of B are two right-hand side vectors (4, 6, 2) and (9, 1, -5):\n",
    "B = np.array([[4,  9],\n",
    "              [6,  1],\n",
    "              [2, -5]])\n",
    "\n",
    "# Solve AX = B for X:\n",
    "X = np.linalg.solve(A, B)\n",
    "print(\"Solution matrix X:\")\n",
    "print(X)\n",
    "\n",
    "# Verifying the solution:\n",
    "verification = A @ X\n",
    "print(\"\\nVerification (A @ X should equal B):\")\n",
    "print(verification, '\\n')\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise:__ In circuit analysis, we often need to solve multiple systems with the same connectivity matrix but different excitations. Consider a circuit with $ 4 $ nodes described by the following (symmetric) nodal admittance matrix:\n",
    "$$\n",
    "Y = \\left[ \\begin{array}{rrrr}\n",
    "10 & -4 & -6 & 0 \\\\\n",
    "-4 & 8 & 0 & -4 \\\\\n",
    "-6 & 0 & 10 & -3 \\\\\n",
    "0 & -4 & -3 & 7\n",
    "\\end{array} \\right]\n",
    "$$\n",
    "Solve for the node voltages given the following three current excitation vectors (right-hand sides):\n",
    "$$\n",
    "\\mathbf{i}_1 = \n",
    "\\begin{bmatrix}\n",
    "18 \\\\ 0 \\\\ 0 \\\\ 0\n",
    "\\end{bmatrix}\n",
    "\\quad\n",
    "\\mathbf{i}_2 = \n",
    "\\begin{bmatrix}\n",
    "0 \\\\ 18 \\\\ 0 \\\\ 0\n",
    "\\end{bmatrix}\n",
    "\\quad\n",
    "\\mathbf{i}_3 = \n",
    "\\begin{bmatrix}\n",
    "0 \\\\ 0 \\\\ 18 \\\\ 0\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ö° $ 1.5 $ Matrix inversion vs. direct solution\n",
    "\n",
    "In theory, we can solve a linear system $A\\,\\mathbf{x} = \\mathbf{b}$ by computing\n",
    "$\\mathbf{x} = A^{-1}\\,\\mathbf{b}$, provided that $ A $ is invertible (i.e.,\n",
    "nonsingular). Recall that the inverse of $ A $ can be found with `linalg.inv`.\n",
    "However, this approach to solving linear systems is usually avoided in practice\n",
    "because it is computationally more expensive than direct solution methods and\n",
    "can be numerically less stable (meaning that small errors can be amplified).\n",
    "\n",
    "Informally, the __condition number__ of a matrix $ A $ measures how sensitive\n",
    "the product $ A \\,\\mathbf{x} $ is to perturbations in the entries of $ A $. It\n",
    "is given by\n",
    "$$\n",
    "\\kappa(A) = \\|A\\|\\  \\|A^{-1}\\|\n",
    "$$\n",
    "where $\\|\\cdot\\|$ denotes a matrix norm. A __well-conditioned__ matrix is one\n",
    "whose condition number is relatively small. Matrices that are nearly singular,\n",
    "i.e., whose determinant lies close to $ 0 $, have very large condition numbers\n",
    "because their inverses will have very large entries. The condition number of\n",
    "a singular matrix is infinite.\n",
    "\n",
    "The condition number of $ A $ can be computed with `linalg.cond(A)`. By\n",
    "default this uses the $ L_2 $ norm (square root of the largest eigenvalue of $\n",
    "A^TA $), though other norms can also be used; see the [documentation](https://numpy.org/doc/stable/reference/generated/numpy.linalg.cond.html).\n",
    "\n",
    "While solving a linear system using inversion or a direct method will yield\n",
    "essentially the same answer for well-conditioned problems, the difference in\n",
    "numerical stability becomes important for ill-conditioned coefficient matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise:__ Compute the condition numbers of the following three matrices:\n",
    "$$\n",
    "\\text{(a)}\\quad A_1 = \\begin{bmatrix}\n",
    "3 & 0 \\\\\n",
    "0 & 1\n",
    "\\end{bmatrix} \\qquad\n",
    "\\text{(b)}\\quad A_2 = \\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "1.001 & 2.001\n",
    "\\end{bmatrix} \\qquad\n",
    "\\text{(c)}\\quad A_3 = \\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "2 & 4\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A1 = np.array([[3, 0],\n",
    "               [0, 1]])  # Nonsingular\n",
    "\n",
    "A2 = np.array([[1, 2],\n",
    "               [1.001, 2.001]])  # Nearly singular\n",
    "\n",
    "A3 = np.array([[1, 2],\n",
    "               [2, 4]])  # Singular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $ \\S 2 $ Least-squares solutions\n",
    "\n",
    "The most general linear system of equations is $ A \\mathbf x = \\mathbf b $ where\n",
    "$$\n",
    "A \\text{ is $ m \\times n $}, \\quad \\mathbf x \\in \\mathbb{R}^n \\quad \\text{and} \\quad \\mathbf b \\in \\mathbb{R}^m\\,. \n",
    "$$\n",
    "Geometrically, a solution $ \\mathbf x $ corresponds to a choice of scalars $ x_k\n",
    "$ that would express $ \\mathbf b $ as a linear combination $ \\mathbf b = x_1\\,\n",
    "\\mathbf a_1 + \\cdots + x_n\\, \\mathbf a_n $ of the $ n $ column-vectors of $ A $:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "b_{1} \\\\\n",
    "b_{2} \\\\\n",
    "\\vdots \\\\\n",
    "b_{m}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "x_1\n",
    "\\begin{bmatrix}\n",
    "a_{11} \\\\\n",
    "a_{21} \\\\\n",
    "\\vdots \\\\\n",
    "a_{m1}\n",
    "\\end{bmatrix}\n",
    "+\n",
    "x_2\n",
    "\\begin{bmatrix}\n",
    "a_{12} \\\\\n",
    "a_{22} \\\\\n",
    "\\vdots \\\\\n",
    "a_{m2}\n",
    "\\end{bmatrix}\n",
    "+\n",
    "x_n\n",
    "\\begin{bmatrix}\n",
    "a_{1n} \\\\\n",
    "a_{2n} \\\\\n",
    "\\vdots \\\\\n",
    "a_{mn}\n",
    "\\end{bmatrix}\\,.\n",
    "$$\n",
    "Thus, there will be a solution if and only if $ \\mathbf b $ happens to lie in\n",
    "the hyperplane $ W \\subset \\mathbb{R}^m $ through the origin spanned by the\n",
    "$ n $ column-vectors of $ A $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the dimension of the subspace $ W $ is at most $ n $, since by definition\n",
    "it is spanned by $ n $ vectors.  Hence if the system is __over-determined__,\n",
    "that is, if we have fewer unknowns than equations ($ n < m $), then $ W $ cannot\n",
    "coincide with all of $ \\mathbb R^m $.  Therefore, in this case, for most choices of\n",
    "$ \\mathbf b \\in \\mathbb R^m $ no exact solution exists. In this situation,\n",
    "_the best that we can do is to pick the vector $ \\mathbf{p} $ in $ W $ that minimizes\n",
    "the distance to $ \\mathbf b $ and to find the corresponding solution $\n",
    "\\mathbf{\\hat{x}} $ to the linear system_\n",
    "$$\n",
    "A\\, \\mathbf{x} = \\mathbf{p}\\,.\n",
    "$$\n",
    "This $ \\mathbf{p} $ is the __orthogonal projection__ of $ \\mathbf b $ onto $ W $. It is the\n",
    "closest vector to $ \\mathbf b $ in $ W $, so that $ \\mathbf{\\hat x} $ is such that the distance\n",
    "from $ A\\,\\mathbf{\\hat x} $ to $ \\mathbf b $ is as small as possible, that is,\n",
    "it is the solution to the optimization problem:\n",
    "$$\n",
    "\\boxed{\\ \\underset{\\mathbf{x}}{\\text{minimize}}\\ \\Vert A\\,\\mathbf{x} - \\mathbf{b} \\Vert^2\\ }\n",
    "$$\n",
    "Due to this interperation, this method of obtaining an approximate solution $ \\mathbf{\\hat x} $\n",
    "to the original system $ A\\mathbf x = \\mathbf b $ is known as the __least-squares method__.\n",
    "In NumPy it is implemented through the `linalg.lstsq` function.\n",
    "\n",
    "üìù We have tacitly assumed in the preceding discussion that the modified system $ A \\mathbf{x} = \\mathbf{p} $\n",
    "has a _unique_ solution $ \\mathbf{\\hat x} $. This will be the case if and only if the column-vectors\n",
    "$ \\mathbf a_1,\\cdots,\\mathbf a_n $ of $ A $ which span $ W $ are _linearly independent_. If this\n",
    "is not the case, then there will be an infinite number of solutions to the modified system. In\n",
    "this situation, one usually picks the solution $ \\mathbf{\\hat x} $ that minimizes the Euclidean norm\n",
    "$ \\Vert \\mathbf{\\hat x} \\Vert $. This is also the default behavior of `linalg.lstsq`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise:__ For the linear system given below:\n",
    "$$\n",
    "\\left\\{\\begin{align*}\n",
    "    x + 2y &= 2 \\\\\n",
    "    3x + 4y &= 5 \\\\\n",
    "    5x + 6y &= 5\n",
    "\\end{align*}\\right.\n",
    "$$\n",
    "\n",
    "(a) Verify by hand that an exact solution does not exist.\n",
    "\n",
    "(b) Show that the least-squares approximate solution is given by\n",
    "$ \\mathbf{\\hat x} = \\big(-1, \\frac{7}{4}\\big) $.  _Hint:_ Set up the coefficient\n",
    "matrix $ A $, the vector $ \\mathbf b $ and apply `linalg.lstsq` as already\n",
    "provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the matrix A and vector b:\n",
    "# A = ...\n",
    "# b = ...\n",
    "\n",
    "# Compute the least squares solution:\n",
    "x_hat, _, _, _ = np.linalg.lstsq(A, b, rcond=None)\n",
    "# This function returns three other values besides the approximate solution, hence the '_'s.\n",
    "print(x_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a system has the same number of equations as unknowns but the associated\n",
    "coefficient matrix is singular, then there are either no solutions\n",
    "or there exist infinitely many solutions. In either situation, we should\n",
    "use `linalg.lstsq` rather than `linalg.solve` to solve it. \n",
    "* If the system has infinitely many solutions, then the result is the\n",
    "  true solution which has minimum euclidean norm among all solutions of\n",
    "  $ A\\,\\mathbf{x} = \\mathbf{b} $.\n",
    "* If the system has no solutions, then least squares yields the minimum-norm\n",
    "  _approximate_ solution, i.e., the solution to $ A\\,\\mathbf{x} = \\mathbf{p} $\n",
    "  which has the smallest euclidean norm among all such solutions. (Here $\n",
    "  \\mathbf{p} $ is the orthogonal projection of $ \\mathbf{b} $ onto the subspace\n",
    "  spanned by the columns of $ A $.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise:__ Using `lstsq`, solve or find approximate solutions to the following systems of equations:\n",
    "$$\n",
    "\\begin{array}{cc}\n",
    "\\text{(a)} \\quad\n",
    "\\left\\{\n",
    "\\begin{array}{rcl}\n",
    "x & + & y & =\\ 2\\\\\n",
    "2x & + & 2y & =\\ 4\n",
    "\\end{array}\n",
    "\\right.\n",
    "& \\qquad\n",
    "\\text{(b)} \\quad\n",
    "\\left\\{\n",
    "\\begin{array}{rcl}\n",
    "x & + & y &=\\ 2\\\\\n",
    "2x & + & 2y &=\\ 2\n",
    "\\end{array}\n",
    "\\right.\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "(c) Find (by hand) _all_ solutions to (a) and verify that the least-squares\n",
    "solution is indeed the one that has minimum norm among all possible solutions.\n",
    "\n",
    "(d) Show (by hand) that (b) has no solutions. Then find the orthogonal\n",
    "projection $ \\mathbf{p} $ of $ \\mathbf{b} = (2, 2) $ onto the subspace spanned\n",
    "by the columns of $ A $, and finally verify that the solution $ \\hat{\\mathbf{x}}\n",
    "$ yielded by `lstsq` is the one having minimum norm among all solutions to\n",
    "$ A\\,\\mathbf{x} = \\mathbf{p} $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $ \\S 3 $ Eigenvalues and eigenvectors\n",
    "\n",
    "Given a square matrix $ A $, an __eigenvector__ $ \\mathbf{v} $ is a\n",
    "nonzero vector such that when $ A $ acts on $ \\mathbf{v} $,\n",
    "the result is a scalar multiple of $ \\mathbf{v} $, that is,\n",
    "$$\n",
    "\\boxed{\\ A\\mathbf{v} = \\lambda\\mathbf{v}\\ }\n",
    "$$\n",
    "for some $ \\lambda \\in \\mathbb{R} $, which is called the __eigenvalue__ corresponding to\n",
    "the eigenvector $ \\mathbf{v} $. NumPy provides the `linalg.eig`\n",
    "function to compute the eigenvalues and eigenvectors of a square matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[0, 0, 6],\n",
    "              [1, 0, -11],\n",
    "              [0, 1, 6]])\n",
    "print(f\"Matrix A:\\n{A}\")\n",
    "# Compute eigenvalues and eigenvectors:\n",
    "eigenvalues, eigenvectors = np.linalg.eig(A)\n",
    "\n",
    "print(\"\\nEigenvalues:\")\n",
    "print(eigenvalues)\n",
    "print(\"\\nEigenvectors (each column corresponds to an eigenvalue):\")\n",
    "print(np.round(eigenvectors, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìù The eigenvectors returned by NumPy are normalized to have unit length (i.e.,\n",
    "$ \\|\\mathbf{v}\\| = 1 $). Moreover, they appear in the same order as their\n",
    "corresponding eigenvalues.\n",
    "\n",
    "Eigenvectors and eigenvalues have numerous applications across fields\n",
    "ranging from quantum mechanics to machine learning. They are also one of the\n",
    "central concepts in linear algebra itself. Some of their\n",
    "most important properties include:\n",
    "\n",
    "* The _determinant_ of a matrix equals the product of its eigenvalues.\n",
    "* The _trace_ of a matrix (sum of diagonal elements) equals the sum of its eigenvalues.\n",
    "* A matrix is _invertible_ if and only if all of its eigenvalues are nonzero.\n",
    "* _Similar matrices_ have the same eigenvalues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise:__ Recall that any real $ n \\times n $ _symmetric_ matrix $ S $ can\n",
    "be diagonalized over $ \\mathbb R $ , meaning that we can find a basis for $\n",
    "\\mathbb R^n $ consisting of eigenvectors of $ S $. Not only that, we can choose\n",
    "an _orthogonal_ basis of eigenvectors.  Check that the matrix below has a full\n",
    "set of eigenvalues and that its eigenvectors are indeed orthogonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = np.array([[2, -1, 0],\n",
    "              [-1, 2, -1],\n",
    "              [0, -1, 2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìù The function `linalg.eigh` is specifically designed to compute the\n",
    "eigenvalues and eigenvectors of a symmetric or Hermitian matrix. (A Hermitian\n",
    "matrix is a complex matrix equal to its conjugate transpose.)\n",
    "It is more efficient than the general `linalg.eig` and guarantees real\n",
    "eigenvalues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise:__ Redo the preceding exercise using `eigh` instead of `eig`. Do you\n",
    "obtain the same results? If not, can they be reconciled (i.e., can you explain the discrepancies)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we only need the eigenvalues, we can use `linalg.eigvals` (or `linalg.eigvalsh` for symmetric matrices).\n",
    "For example, the matrix\n",
    "$$\n",
    "R = \\begin{bmatrix}\n",
    "0 & -1 \\\\\n",
    "1 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "corresponds to a counter-clockwise rotation of $ \\mathbb{R}^2 $ by $\n",
    "\\frac{\\pi}{2} $ radians ($ 90 $ degrees).  Its eigenvalues are $ \\pm i $, where\n",
    "$ i $ is the imaginary unit. We can check this with the help of NumPy (but\n",
    "recall that in Python, the imaginary unit is denoted by `j`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = np.array([[0, -1],\n",
    "              [1, 0]])\n",
    "\n",
    "print(np.linalg.eigvals(R))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise (Markov chains):__\n",
    "A _Markov chain_ is a mathematical system that undergoes transitions from one\n",
    "state to another according to certain probabilistic rules. In a Markov chain,\n",
    "the probability distribution of the next state depends only on the current\n",
    "state. The _steady-state distribution_ is the long-run proportion of time the\n",
    "Markov chain spends in each state, regardless of the initial state. \n",
    "\n",
    "Consider a simple Markov chain representing transitions between\n",
    "three weather states: sunny, cloudy, and rainy, or states $ 0 $, $ 1 $ and $ 2\n",
    "$, respectively. The _transition matrix_ $ P $ is given by\n",
    "$$\n",
    "P = \\begin{bmatrix}\n",
    "0.7 & 0.2 & 0.1 \\\\\n",
    "0.3 & 0.4 & 0.3 \\\\\n",
    "0.2 & 0.3 & 0.5\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "where entry $ P_{ij} $ is the probability of transitioning from state $ i $ to\n",
    "state $ j $.\n",
    "\n",
    "(a) Find the eigenvalues and eigenvectors of $ P^T $ (the transpose of $ P $).\n",
    "\n",
    "(b) Use the eigenvector of $ P^T $ corresponding to eigenvalue $ 1 $ to\n",
    "determine the long-term probability distribution $ \\mathbf{s} $ of weather\n",
    "states (i.e., the steady-state distribution); this can be obtained by\n",
    "scaling this eigenvector so that the sum of coordinates becomes $ 1 $.\n",
    "\n",
    "(c) Verify your answer by raising the transition matrix $ P $ to a high power\n",
    "$ n $: It can be shown that as $ n \\to \\infty $, each row of $ P^n $ converges\n",
    "to $ \\mathbf{s} $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = np.array([\n",
    "    [0.7, 0.2, 0.1],  # Sunny -> Sunny, Cloudy, Rainy\n",
    "    [0.3, 0.4, 0.3],  # Cloudy -> Sunny, Cloudy, Rainy\n",
    "    [0.2, 0.3, 0.5]   # Rainy -> Sunny, Cloudy, Rainy\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $ \\S 4 $ SciPy for Solving Equations\n",
    "\n",
    "__SciPy__ is a library built on top of NumPy that provides extensive\n",
    "functionality for scientific computing. It is one of the most important packages\n",
    "in the Python world. Because of its broad scope, we cannot hope to cover it in\n",
    "detail here. Instead, we will focus on presenting the basic tools to solve\n",
    "general (nonlinear) equations and optimization problems.\n",
    "\n",
    "### $ 4.1 $ SciPy's linear algebra module\n",
    "\n",
    "SciPy has a `linalg` module that overlaps substantially with NumPy's, but\n",
    "also provides more advanced linear algebra routines, including\n",
    "specialized solvers for various types of matrices (symmetric, banded,\n",
    "triangular, etc.) and direct solvers for both dense and sparse systems. Since\n",
    "these involve more specialized topics, we will not go into them here.\n",
    "Instead, let's compare in a simple example how both NumPy and SciPy can solve\n",
    "linear systems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[4, 2, 1],\n",
    "              [2, 5, 3],\n",
    "              [1, 3, 6]])\n",
    "b = np.array([7, 10, 8])\n",
    "\n",
    "# NumPy solution:\n",
    "x_np = np.linalg.solve(A, b)\n",
    "print(\"NumPy solution:\")\n",
    "print(x_np)\n",
    "\n",
    "# SciPy solution:\n",
    "x_sp = spla.solve(A, b)\n",
    "print(\"\\nSciPy solution:\")\n",
    "print(x_sp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $ 4.2 $ Solving a single nonlinear equation\n",
    "\n",
    "SciPy's `optimize` module provides functions for solving nonlinear equations.\n",
    "One of its procedures is `root_scalar`, which we can use to find a root (zero)\n",
    "of functions of a single variable. In order for `root_scalar` to do its job, we\n",
    "must provide either:\n",
    "* a __bracketing interval__ $ [a, b] $, that is, an interval\n",
    "within the domain of the function such that $ f(a) $ and $ f(b) $ have opposite\n",
    "signs; or\n",
    "* an initial guess $ x_0 $ at a root plus an argument specifying which\n",
    "  root-finding method to use (such as Newton's).\n",
    "\n",
    "For instance, consider the problem of locating a root of the cubic polynomial\n",
    "$$\n",
    "f(x) = x^3 - 2x^2 + 4x - 8\\,.\n",
    "$$\n",
    "One easily checks that\n",
    "$$\n",
    "f(1) = 1 -2 + 4 - 8 < 0 \\qquad \\text{while} \\qquad f(3) = 27 - 18 + 12 - 8 > 0\n",
    "$$\n",
    "Hence by the intermediate value theorem there must exist a root of $ f $ inside\n",
    "$ [1, 3] $. Let's check this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: x**3 - 2*x**2 + 4*x - 8  # defining f\n",
    "\n",
    "# Using root_scalar with bracket method:\n",
    "solution = spo.root_scalar(f, bracket=[1, 3])\n",
    "print(\"Root found:\")\n",
    "print(round(solution.root, 4))\n",
    "print(\"Function value at the root:\")\n",
    "print(round(f(solution.root), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The return value of `root_scalar` contains other useful information besides the\n",
    "value of the root:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the answer by plotting the graph of $ f $:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.linspace(0, 3, 1000)\n",
    "ys = f(xs)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.plot(xs, ys)\n",
    "ax.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "ax.grid(linestyle=\"--\", alpha=0.3)\n",
    "ax.plot(solution.root, 0, 'ro', markersize=6)\n",
    "ax.set_xlabel(\"$ x $\")\n",
    "ax.set_ylabel(\"$ f(x) $\")\n",
    "ax.set_title(\"Find a root of $ f(x) = x^3 - 2 x^2 + 4 x - 8 $\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìù `root_scalar` doesn't find all the roots, it finds a single root near an initial\n",
    "guess or within a specified bracketing interval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise:__ Solve the following nonlinear equation using `root_scalar` and a\n",
    "bracketing interval.\n",
    "$$ e^x - 5x = 2 $$\n",
    "Make a plot of the function that you used to find a root and mark the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use `root_scalar` with a guess, we also need to specify a method that doesn't\n",
    "require bracketing, such as Newton's method (`newton`) or the secant method\n",
    "(`secant`). Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: x**2 - 4\n",
    "\n",
    "# Use root_scalar with a guess:\n",
    "result = spo.root_scalar(f, \n",
    "                         x0=1.0,           # Initial guess\n",
    "                         method=\"newton\")  # Method that uses a guess\n",
    "\n",
    "print(f\"Root: {round(result.root, 4)}\")\n",
    "print(f\"Function value at root: {round(f(result.root), 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise:__ What happens if you try to find a root of $ g(x) = x^2 + 1 $?\n",
    "Make sure to display the full information on the solution, not just its value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìù `root_scalar` has several optional parameters and other solvers that we haven't mentioned; for full details,\n",
    "please refer to the [documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.root_scalar.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $ 4.3 $ Finding roots of polynomials\n",
    "\n",
    "Polynomials have special properties that permit us to use more efficient and accurate\n",
    "root-finding algorithms. Given a polynomial\n",
    "$$\n",
    "p(x) = a_n x^n + a_{n-1} x^{n-1} + \\cdots + a_1 x + a_0\\,,\n",
    "$$\n",
    "we can find all of its roots using NumPy's `roots` function, which takes the\n",
    "array $ [a_n, a_{n-1}, \\ldots, a_1, a_0] $ of coefficients (in this order) as\n",
    "input. As an illustration, consider the problem of finding the roots of\n",
    "$$\n",
    "p(x) = x^3 - 2x^2 - 5x + 6\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs = np.array([1, -2, -5, 6])  # coefficients from highest to lowest degree\n",
    "roots = np.roots(coeffs)\n",
    "print(\"Roots of polynomial:\", roots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise:__ Verify that these are indeed all the roots of $ p $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise:__ Determine all roots of\n",
    "$$\n",
    "q(x) = x^4 - 17x^3 + 101x^2 - 247x + 210\\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $ 4.4 $ Solving a system of nonlinear equations\n",
    "\n",
    "Consider the\n",
    "problem of finding the intersection between the circle of radius $ 1 $ centered\n",
    "at $ \\big(1, \\frac{1}{2}\\big) $  and the graph of the sine curve. In other\n",
    "words, we want to solve the (nonlinear) system of equations\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\ (x - 1)^2 + (y - 0.5)^2 = 1 \\\\\n",
    "\\ y = \\sin x\n",
    "\\end{cases}\n",
    "$$\n",
    "Let's make a plot of the situation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "# Plot the sine curve:\n",
    "xs = np.linspace(-1, 3, 1000)\n",
    "ax.plot(xs, np.sin(xs), \"r-\", label=\"Sine: $ y = \\\\sin(x) $\")\n",
    "# Plot the circle:\n",
    "theta = np.linspace(0, 2*np.pi, 100)\n",
    "circle_xs = 1 + np.cos(theta)\n",
    "circle_ys = 0.5 + np.sin(theta)\n",
    "ax.plot(circle_xs, circle_ys, \"b-\", label=\"Circle: $ (x-1)^2 + (y-0.5)^2 = 1 $\")\n",
    "\n",
    "ax.set_xlabel(\"$ x $\")\n",
    "ax.set_ylabel(\"$ y $\")\n",
    "ax.set_title(\"Intersection of a circle and sine curve\")\n",
    "ax.grid(linestyle=\"--\", alpha=0.3)\n",
    "ax.axis(\"equal\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such systems can be solved with `scipy.optimize.root` by providing an initial\n",
    "guess. Here's how we can find both points of intersection in our example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the system of equations:\n",
    "def equations(vars):\n",
    "   x, y = vars\n",
    "   eq1 = (x - 1)**2 + (y - 0.5)**2 - 1  # Circle equation, with RHS = 0\n",
    "   eq2 = y - np.sin(x)                  # Sine curve equation, with RHS = 0\n",
    "   return [eq1, eq2]\n",
    "\n",
    "# Find intersections using different initial guesses:\n",
    "initial_guesses = [\n",
    "   [0, 0],   # Guess for left intersection\n",
    "   [2, 1]    # Guess for right intersection\n",
    "]\n",
    "\n",
    "# Solve for each initial guess:\n",
    "for i, guess in enumerate(initial_guesses):\n",
    "   solution = spo.root(equations, guess)\n",
    "   \n",
    "   print(f\"\\nSolution {i+1} (from initial guess at {guess}):\")\n",
    "   print(f\"x, y = {np.round(solution.x, 4)}\")\n",
    "   print(\"Function values at the solution:\")\n",
    "   print(f\"f(x,y) = {np.round(solution.fun, 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise:__ Find the intersection points of the curves described by the\n",
    "following two nonlinear equations.\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\ \\sin x + \\cos y = 0.5 \\\\\n",
    "\\ x^2 + y^2 = 1\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $ 4.5 $ Optimization problems\n",
    "\n",
    "SciPy's `optimize` module also provides functions for solving __optimization__\n",
    "problems, that is, the task finding maxima and minima of a function (usually\n",
    "called an _objective function_ in this context), possibly with some additional\n",
    "constraints.\n",
    "\n",
    "Let's look at the problem of minimizing the function\n",
    "$$\n",
    "f(x, y) = x^2 + y^2 + (x + y - 2)^2\\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to minimize:\n",
    "f = lambda vars: vars[0]**2 + vars[1]**2 + (vars[0] + vars[1] - 2)**2\n",
    "\n",
    "# Initial guess:\n",
    "x0 = [0, 0]\n",
    "\n",
    "# Minimize the function:\n",
    "result = spo.minimize(f, x0, method=\"BFGS\")\n",
    "\n",
    "print(\"Minimum found at:\")\n",
    "print(result.x)\n",
    "print(\"\\nFunction value at minimum:\")\n",
    "print(result.fun)\n",
    "print(\"\\nNumber of iterations:\")\n",
    "print(result.nit)\n",
    "print(\"\\nConvergence message:\")\n",
    "print(result.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise:__ Check by hand that $ (2/3, 2/3) $ is the only critical point of\n",
    "$ f(x) $, and that this is indeed a global minimum.\n",
    "\n",
    "Let's check this answer visually by plotting the contour lines of $ f $ and\n",
    "marking the minimum point found by SciPy. (We will focus on contour and other\n",
    "types of plots in a future notebook; for now, you can ignore the code below and\n",
    "focus on the plot itself.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.linspace(-2, 2, 100)\n",
    "ys = np.linspace(-2, 2, 100)\n",
    "X, Y = np.meshgrid(xs, ys)\n",
    "Z = X**2 + Y**2 + (X + Y - 2)**2\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "contour = ax.contour(X, Y, Z, 20, cmap=\"viridis\")\n",
    "fig.colorbar(contour, ax=ax)\n",
    "ax.scatter(result.x[0], result.x[1], color=\"red\", s=100, marker=\"x\")\n",
    "ax.grid(True)\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.grid(linestyle=\"--\", alpha=0.3)\n",
    "ax.set_title(\"Contour Plot of $ x^2 + y^2 + (x + y - 2)^2 $\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìù By default, `minimize` tries to find a _local_ minimum, not necessarily the\n",
    "_global_ minimum, close to your initial guess. To find maxima, we can call\n",
    "`minimize` on the negative of the function we want to maximize.\n",
    "\n",
    "__Exercise:__ Find a (local) maximum of \n",
    "$$\n",
    "f(x, y) = x\\sin(4x) + y\\sin(2y)\\,.\n",
    "$$\n",
    "Experiment with several different initial guesses and check whether the\n",
    "results change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
